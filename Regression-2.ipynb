{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f3cf44-5c9b-402c-804f-a950c76d87f6",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714f5f1-4906-471a-a82d-dfc15f488f89",
   "metadata": {},
   "source": [
    "R-squared, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It ranges from 0 to 1, where 0 indicates that the model explains none of the variability and 1 indicates that the model explains all the variability.\n",
    "\n",
    "R-squared is calculated as:\n",
    "\n",
    "R² = 1 - (SS_res / SS_tot)\n",
    "\n",
    "where SS_res is the sum of the squared residuals and SS_tot is the total sum of squares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efd879-aa6c-4aee-89bb-6f163b3b63bb",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e70f36-0736-4611-9295-afdab5edee90",
   "metadata": {},
   "source": [
    "Adjusted R-squared adjusts the R-squared value based on the number of predictors in the model. Unlike R-squared, which can only increase as more predictors are added, adjusted R-squared can decrease if the added predictors do not improve the model sufficiently.\n",
    "\n",
    "Adjusted R-squared is calculated as:\n",
    "\n",
    "Adjusted R² = 1 - [((1 - R²) * (n - 1)) / (n - k - 1)]\n",
    "\n",
    "where n is the number of observations and k is the number of predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adecd915-82c4-4972-92ef-f122988c6043",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b6d92-240b-451c-8e15-0755924ed156",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use when comparing models with different numbers of predictors. It provides a more accurate measure of model performance by penalizing the addition of irrelevant predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f192673-bbcf-48ee-af1d-7512adc3ae0a",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bee318-8683-4277-9ca3-2fd91277771d",
   "metadata": {},
   "source": [
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are metrics used to evaluate the performance of regression models.\n",
    "\n",
    "- MSE is the average of the squared differences between the predicted and actual values.\n",
    "- RMSE is the square root of the MSE, providing the error in the same units as the target variable.\n",
    "- MAE is the average of the absolute differences between the predicted and actual values.\n",
    "\n",
    "Formulas:\n",
    "MSE = (1/n) * Σ(y_i - ŷ_i)²\n",
    "RMSE = √MSE\n",
    "MAE = (1/n) * Σ|y_i - ŷ_i|\n",
    "\n",
    "where y_i are the actual values, ŷ_i are the predicted values, and n is the number of observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffa55b-2926-47bf-97be-f086b4bd8ffa",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057b0cb-bc2b-4501-9b7f-14a060520b02",
   "metadata": {},
   "source": [
    "Advantages of RMSE, MSE, and MAE:\n",
    "- RMSE: Sensitive to large errors due to squaring the differences.\n",
    "- MSE: Provides a clear measure of the average squared error.\n",
    "- MAE: Less sensitive to outliers compared to RMSE and MSE.\n",
    "\n",
    "Disadvantages:\n",
    "- RMSE and MSE: Can be overly sensitive to outliers.\n",
    "- MAE: Does not penalize larger errors as much as RMSE and MSE.\n",
    "\n",
    "Use RMSE when you want to penalize larger errors more. Use MAE when you want a metric less sensitive to outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d7c52-af17-4ac4-825a-690607989e67",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13622b11-ea37-40dd-9b92-35430ddcbcfc",
   "metadata": {},
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function, encouraging sparsity in the model.\n",
    "\n",
    "Ridge regularization adds a penalty equal to the square of the magnitude of coefficients to the loss function, shrinking coefficients but not necessarily to zero.\n",
    "\n",
    "Use Lasso when you want feature selection and a sparse model. Use Ridge when you want to prevent overfitting without necessarily reducing the number of predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12cbef-a0bf-44d9-830e-16a91153c762",
   "metadata": {},
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dd2df8-b5d7-4b6a-b9b7-aecd6d401d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.002379396707439468, 3.265306119027794e-05)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regularized linear models, such as Lasso and Ridge regression, add a penalty to the loss function to constrain the size of the coefficients. This prevents overfitting by discouraging complex models that fit the noise in the training data.\n",
    "\n",
    "#Example: Regularizing a linear model to predict house prices.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = {'Size': [1500, 1600, 1700, 1800, 1900],\n",
    "        'Bedrooms': [3, 3, 4, 4, 5],\n",
    "        'Price': [300000, 320000, 340000, 360000, 380000]}\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['Size', 'Bedrooms']]\n",
    "y = df['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "\n",
    "lasso = Lasso(alpha=1.0)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
    "\n",
    "ridge_mse, lasso_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb6e59-2e33-49e3-8fe4-34e05db9486a",
   "metadata": {},
   "source": [
    "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d89ded-9086-449e-887f-6f7f282e6c5d",
   "metadata": {},
   "source": [
    "Limitations of regularized linear models:\n",
    "1. Can be sensitive to the choice of regularization parameter.\n",
    "2. May not perform well if the underlying relationship between variables is highly non-linear.\n",
    "3. Lasso can arbitrarily select one of the correlated predictors while ignoring others, leading to instability.\n",
    "\n",
    "Regularized linear models may not be the best choice when:\n",
    "1. The dataset is small and the risk of overfitting is low.\n",
    "2. The relationship between predictors and the target variable is highly non-linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1565c-d8ed-4d24-befe-d93c0febe910",
   "metadata": {},
   "source": [
    "## Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec3acd-a4eb-4100-9ce5-ed54ec6f5b1f",
   "metadata": {},
   "source": [
    "Choosing between Model A and Model B depends on the context:\n",
    "\n",
    "- RMSE penalizes larger errors more than MAE. If larger errors are particularly undesirable, Model A (with RMSE) might be preferred.\n",
    "- MAE is less sensitive to outliers and provides a straightforward interpretation of the average error.\n",
    "\n",
    "Limitations:\n",
    "- Comparing RMSE and MAE directly is not straightforward as they measure different aspects of model performance.\n",
    "- RMSE is in the same units as the target variable, making it easier to interpret in context.\n",
    "\n",
    "Without more context, it's challenging to definitively choose the better performer. Consider the distribution of errors and the specific needs of the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5dd00c-9266-42ad-b5d2-53cfd7f77b32",
   "metadata": {},
   "source": [
    "## Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e209d17-c485-4847-9544-ef760105ac98",
   "metadata": {},
   "source": [
    "Choosing between Model A (Ridge) and Model B (Lasso) depends on the specific goals:\n",
    "\n",
    "- Ridge regularization (Model A) is better if you want to keep all features and prevent overfitting.\n",
    "- Lasso regularization (Model B) is better if you want feature selection and a sparser model.\n",
    "\n",
    "Trade-offs and limitations:\n",
    "- Ridge does not perform feature selection and may retain irrelevant features.\n",
    "- Lasso can lead to instability by arbitrarily selecting one of the correlated features while ignoring others.\n",
    "- The choice of regularization parameter significantly impacts model performance.\n",
    "\n",
    "Evaluate both models using cross-validation and consider the context of the application before making a decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622523aa-75ac-42ce-9eeb-1e2f0ced32e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
